From the beginning of civilization, mapping the surroundings has been a key concept for navigating the environment. In essence, the problem our ancestors faced is currently being tackle in robotics: How do we map the environment and know our location within it? This is a massive problem to tackle and of extreme importance to achieve full robot autonomy. In a simple approach, a depth sensor in a static platform is enought to map the surroundings, this is the base platform of the taxometry chart presented in Figure \ref*{fig: taxonomy graph}. At this level a localization module is not needed, but the amount of uses in real world are skew. The next level presents the localization component, this allows the usage of mobile robot's to map the environment. If the localization information is correct, precise and without any uncertainty associated this would be enough to generate an accurate map. However, this scenario is not reallistic. The more complex level presents a two way communication between the mapping and localization, this is often refered as \acl*{SLAM} (\acs*{SLAM}). \acs*{SLAM} is the challenge of mapping the local environment perceived by a moving entity (e.g. robot) and updating the map and localization simultaneously and continuously as the entity moves through space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{images/background/mapping taxo.pdf}
    \caption{Mapping taxonomy graph}
    \label{fig: taxonomy graph}
\end{figure}


\section{Mapping}

Maps in robotics are usually represented either metrically or topologically \cite{thrun_learning_1998}. The information on topological maps is presented in the form of a graph that contains all of the important landmarks. There are many advantages to topological maps, such as space efficiency, but they are often more difficult to construct in large environments if sensory information is suboptimal and landmarks are hard to identify \cite{thrun_learning_1998}. In the metric representation, everything above the resolution of the system and map appears on the map. A metric map can be represented with raw sensor measurements or occupancy grids, the latter being more common. In occupancy grids, each cell holds the probability that the space in that cell is currently occupied. As the cell value increases, the likelihood of that space being occupied also increases. The resolution of the map will be determined by the size of the grid, therefore to generate a precise map, one must have large dedicated memory space. While metric maps are easy to built and mantain, they tend to be space consumming \cite{thrun_learning_1998}. To combat the large amount of space needed to store metric maps, the Octomap library was developed. 
Octomap is a popular 3D mapping framework to generate occupancy grids based on depth information \cite{hornung_octomap_2013}. Octomap uses octrees\footnote{An octree is a tree data struture in which each internal node is subdivided in eight children until the structure resolution is met.} to help reduce the memory space of rendering a map since large spaces with equal values can be represent with a single parent node.

\section{Localization}
Localization can take two forms: global or relative. Global localization gives the position in a global reference. Information provided by global localization can be viewed independently of other positions. As the name implies, relative localization establishes a relative relationship between the current pose and other poses, usually determined by consecutive states. The procedure of estimating a change in pose over time by using sensory information is called odometry. Classical odometry is computed from motion sensors such as \acs*{IMU} or wheel encoders, but it is also possible to use cameras as an input to acquire odometry, this is called visual odometry. Feature matching algorithms (identifying and relating the same features of an object from different perpectives) such as SURF \cite{leonardis_surf_2006} or ORB \cite{rublee_orb_2011} are at the core of many Visual Odometry algorithms. Recently, Liu et al. \cite{liu_visual_2021} developed a Visual Odometry algorithm based on a Deep Learning technique.
It is also possible to use \acs*{LiDAR} to acquire odometry and scan matching algorithms like \acl*{ICP} method (\acs*{ICP}) are often used. Scan matching takes two pointclouds or 2D \acs*{LiDAR} scans and finds the transformation that produces the least amount of error between points.

Simply put, it tries to transform in a way to overlap the information of both scans, the transform is then applied to estimate the robots location, as is shown in Figure \ref*{fig: scan mathing algorithm}. Both scan matching and feature matching can be used to find a transform to update the localization of the robot, Figure \ref*{fig: scan mathing algorithm} shows how scan matching can be used to update the robot location with information of the map.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/background/Scan-matching-problem-definition-a-Scans-before-alignment-b-Scans-after-alignment.png}
    \caption{Scan matching to calculate a robot position. \cite{konecny_scan_2019}}
    \label{fig: scan mathing algorithm}
\end{figure}

When computing relative localization, in each iteration odometry information is combined with the previous pose, both of which are subject to uncertainty. Over time, the cumulative error of the robot will increase significantly. One way to reduce this is to clean the sensory information and merge both relative and global localization, which is usually done by Kalman Filters.

\subsubsection{Kalman Filter}
The Kalman filter is a \textbf{linear} iterative process based on Bayes Theorem, that uses consecutive data input to quickly converge to the true value. Each iteration involves computing three values: the Kalman gain, the current estimate, and its uncertainty. The estimate of the current iteration is computed with the new data input and the previous estimation, where the weight of each component is decided by the kalman gain. Once the current estimate has been calculated, the new uncertainty of the estimate is computed. One can think of the Kalman Gain as a variable that represents the confidence one has in the observations and predictions made. Figure \ref*{fig: flowchart kalman} provides a simple graphical overview of this process. A more complete explanation of the statistics envolved in the Kalman filter is provided in Annex. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/background/Kalman-diagram.pdf}
    \caption{Simple flowchart of the Kalman Filter.}
    \label{fig: flowchart kalman}
\end{figure}

In real world it is uncommon to have a global linear system and due to it's linearity, the Kalman Filter won't work well in nonlinear scenarious. A simple way to solve this is to make the global nonlinear function, locally linear, using a first order Taylor Expansion to do the aproximation. This is the approach taken by the \acl{EKF} (\acs*{EKF}) method. A more accurate way to solve the problem is to use a \acl*{UKF} (\acs*{UKF}). Instead of linearizing the original function, the \acs*{UKF} uses an Unscented Transformation\footnote{The Unscened transform picks a few points from the distribution of the input variable, then it passes them through the nonlinear function and computes the mean and standard deviation of the output. A complete explanation is given in Annex}. A problem still remains, the noise is assumed to follow a gaussian distribution in every Kalman filter approach, which is not necessarely true.

\section{\acs*{SLAM} techniques}

Now that both localization and mapping were reviewed, the techniques to merge them together in \acs*{SLAM} can be presented. As mentioned before, sensors used to capture information of the real world have uncertainties associated with each measurement. Even when algorithms are implemented to diminish these errors, they never vanish completely. When performing \acs*{SLAM}, a moving robot's perception and performance can be severely affected by these inaccuracies as they accumulate and become increasingly large. 

\subsubsection{Loop Closure}

The capacity of the system to recall previously visited landmarks is regarded as loop closure. Loop closure identifies a previously mapped location and through feature or scan matching algorithms updates the robot location. This will significantly reduce the error in localization, since the cumulative error in pose is discarded. One can think of loop closure as a reset of the pose uncertainty whenever the robot passes through a previously known location. However, an inaccurate detection of a loop closure will introduce a large error in both the mapping and robot's localization, so one must be sure the landmarks are properly identified.

\subsection{Filter based \acs*{SLAM}}

Filtering based \acs*{SLAM} uses the prevously described filtering techniques. The \acs*{EKF} \acs*{SLAM} is one way to solve the \acs*{SLAM} problem using the previously explained \acl*{EKF}. As it uses the \acs*{EKF}, each map and pose must linearly dependent of the previous iteration and this is major shortcome. If the robot moves at a constant speed, the change in pose can be assumed linear. This will not be the case as the robot often accelerates, but if the acceleration is not abrupt the nonlinear system will be succefully approximated to linear. The biggest drawback is the implied assumption of a static map. Moving entities in the environment will obviously not produce a close to linear dependency in succeful iterations, therefor the first order Taylor expansion will not yield good results. 


The FastSLAM algorithm \cite{thrun_fastslam_nodate} uses particle filters to estimate the change in pose of the robot. By using Particle Filters, FastSLAM can handle nonlinear robot motion but it is more computational intensive. Another advantage of using a Particle Filter to estimate the motion is the multiple system states that are kept in memory through the resampling process, that only discards the low probability states. There are also implementations which replace the \acs*{EKF} by the \acs*{UKF} method to update the landmarks \cite{wang_upf-ukf_2007}, that produce more accurate results but are usually more computational intensive.


\subsection{Graph-based \acs*{SLAM}}

The graph-based \acs*{SLAM} technique, initially proposed by Lu and Milios et al. \cite{lu_globally_1997}, uses a graph-based approach, in which nodes represent poses or landmarks, while edges represent constraints between nodes, typically obtained from odometry or loop closure techniques, as illustrated in Fig. \ref*{fig: graph SLAM}. Unlike the filtering approach, the graph-based technique maintains knowledge on all gathered data and uncertainties are encoded in the graph. This approach divides \acs*{SLAM} into two main problems: graph construction (building the graph from sensory information, commonly known as front-end) and graph optimization (analysing and estimating the most likely global configuration of poses given the constraints, also known as back-end).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/background/graph_SLAM.png}
    \caption{A pose-graph representation of \acs*{SLAM}. The circles represent the robot's poses, the solid lines are constrains derived from odometry and the dotted lines are from scan matching constrains \cite{lu_globally_1997}.}
    \label{fig: graph SLAM}
\end{figure}

Every new pose introduces a new node into the graph with multiple constraints associated, the constrains can be divided into \textit{weak links} provided by odometry data and \textit{strong links} obtained from scan or feature matching algorithms. These links can be viewed as elastic springs with different elastic constants \cite{lu_globally_1997}, the global graph optimizer will then find the lowest energy solution to the system. To put it simply, the frontend is responsible for generation of accurate submaps, while the backend ensures they are consistently correlated.

\section{\acs{ROS}}

An effective robotics project cannot be achieved by just having sensors and physical components; rather, one must have a clever communication system between sensors and processes. This task can be carried out by the \acl*{ROS} (\acs*{ROS}). 

Although the name \acl*{ROS} suggests that \acs*{ROS} is an operating system, this is not the case.  In a way, \acs*{ROS} is both middleware and a framework. The system provides a communication channel where messages can be subscribed, published and distributed, allowing quick integration between systems and components. Moreover, it provides features such as debugging, visualization, testing, logging, and configuration right out of the box. Additionally, ROS includes a number of useful packages for essential areas relevant to robotics such as movement, perception, and manipulation. The ROS community is also constantly evolving with the most recent developments in robotics, so libraries are always being added to ROS. Many of these libraries are open-source implementations of previously described \acs*{SLAM} techniques, some of these libraries will be reviewed and compared in the following section.


\section{\acs*{SLAM} implementations in \acs*{ROS}}

There are a multitude of open source implementations of \acs*{SLAM}. Since one of the \textbf{Must} requirements presented in the MoSCoWW analysis of Section \ref*{chap:introduction} is a well integrated system with the \acs*{ROS} platform, only \acs*{ROS} implementations will be reviewed. 

One of the most popular implementations of \acs*{SLAM} is Real Time Appearance Based Mapping (RTAB-Map) \cite{labbe_rtab-map_2019}. RTAB-Map\footnote{The \acs*{ROS} implementation can be found at \url{https://github.com/introlab/rtabmap}} is a graph-based \acs*{SLAM} that uses loop closure detector and offers several options for the backend, namely GTSAM (default) \cite{gtsam}, g2o \cite{kummerle_g2o_2011} and TORO \cite{grisetti_nonlinear_2009}. It supports several sensors, namely \acs*{LiDAR}, RGB-D and stereo cameras. It can gather odometry from \acs*{IMU} and wheel encoders but it also supports Visual and \acs*{LiDAR} odometry as optional sources. In order to ensure offline\footnote{In literature, real-time \acs*{SLAM} is referred to as offline \acs*{SLAM} and heavy implementations that are not meant to run in real-time are referred as online \acs*{SLAM}} \acs*{SLAM}, RTAB-Map has a memory manager that keeps control of the amount of locations and landmarks used in the loop closure. Additionally, when executing the loop closure, RTAB-Map will reuse the features that were previously matched in Visual or \acs*{LiDAR} Odometry, this also contributes to improve performance. RTAB-Map can generate both 2D and 3D Occupancy grids with Octomap \cite{hornung_octomap_2013}.

Several \acs*{LiDAR} based method derive from \acl*{LOAM}, commonly known as \acs*{LOAM}. Implementations based on \acs*{LOAM} usually use \acs*{LiDAR} to obtain both odometry and mapping. Although it can create high accuracy maps, they are static and it usually performs poorly in places with few landmarks, like long corridors. The LeGO-LOAM\footnote{The \acs*{ROS} package can be at \url{https://github.com/RobustFieldAutonomyLab/LeGO-LOAM}} add two additional modules to the \acs*{LOAM} technique: pointcloud segmentation and loop closure. These extra components allow an improvement in computing performance and drift reduction over long distances but doesn't improve the results when used in a featureless enviroment. LeGO-LOAM uses the naive \acs*{ICP} algorithm to perform loop closure, but a more robust approach, based in a point cloud descriptor is implemented in the SC-LeGO-LOAM\footnote{The \acs*{ROS} package can be at \url{https://github.com/irapkaist/SC-LeGO-LOAM}} \cite{kim_scan_2018}. To help improving the performance in low features environment, researchers started to implement an \acs*{IMU} to the system in a tighly coupled approach \cite{ye2019tightly, xu_fast-lio_2021,xu_fast-lio2_2021}. As it was very computational intensive to take all sensory data, the authors in \cite{liosam2020shan} sugested the \acl*{LIO-SAM} (\acs*{LIO-SAM}\footnote{The \acs*{ROS} package can be at \url{https://github.com/TixiaoShan/LIO-SAM}}).

Cartographer is Google's implementation to solve the \acs*{SLAM} challenge \cite{hess_real-time_2016}. It is another \acs*{LiDAR} based graph \acs*{SLAM} divided into two main components: local \acs*{SLAM} (the front end) and global \acs*{SLAM} (the backend). This approach takes input of range finding sensor, like \acs*{LiDAR}, and applies a bandpass filter to the input data. \acs*{IMU} can also be used to help figuring out the robot rotation and to provide information on gravity direction, that is used in the 3D variant.
 
\section{Related Systems for data acquisition}

It is no surprise that there are already a number of portable and light sensors designed to collect data about the environment around us. There are several proprietary solutions available on the market for gathering sensory information for \acs*{SLAM}, but they tend to be expensive \cite{libackpack_C50, libackpack_DGC50}. Additionally, there are some articles and research papers that have been conducted with the goal of building a system similar to the one proposed here. Considering that the methodology and results of these studies are readily available, these will be the ones that are selected for further study.

There exist multiple ways to scan a forest, Oveland et al. \cite{oveland_comparing_2018} compared three different ground LiDAR scanning implementations, namely, Handeld LiDAR, Backpack LiDAR (composed by two \acs*{LiDAR} perpendicular to each other) and Terrestial LiDAR. The authors compared the measurements of \acs*{DBH} to conclude that while the Terrestial LiDAR Scanners (a fixed system) is the most accurate, it fails to detect ocult areas. According to the authors the handheld LiDAR scanner has troubles in detecting smaller trees. Even though the backpack LiDAR has the highest number of false trees, the authors still aknowledge it as the most efficient method since it presents the highest amount of trees detected and low \acl*{RMSE} (\acs*{RMSE}) of $0.022m$.

Recently, Kui Xao developed a dual \acs*{LiDAR}s system, an \acs*{IMU}, and \acs*{GPS} for performing \acs*{SLAM} in multi-scene applications \cite{xiao_high-precision_2022}. To increase the vertical \acl*{FOV} (\acs*{FOV}), one of the \acs*{LiDAR}s was placed in the \textit{XY} plane while the other was positioned at -77.94° from the \textit{XY} plane. A timestamp synchronization algorithm helped the authors merging the 3D scans from both \acs*{LiDAR}s. Secondly, the \acs*{LiDAR} data is tightly coupled with the IMU data in order to reduce noise caused by the inaccuracy of \acs*{LiDAR}-based odometry. In outdoor tests, the \acs*{IMU} calibration is improved by loosely coupling \acs*{GPS} to the \acs*{IMU}. While their framework outperformed the LeGO-LOAM and LIO-SAM in indoor enviroment it failed to produce any meaningfull diference in an outdoor setup. In forest environments, on the other hand, can be expected to show a slight improvement due to the detection by the vertical \acs*{LiDAR} of the high trees, adding additional features.
Based on an Ouster OS0-128 \acs{LiDAR} and a RealSense D435i, both with an integrated \acs{IMU}, Alexander Proudman's system performs online \acs{SLAM} and estimates \acl{DBH} (\acs*{DBH}) using the data collected \cite{proudman_online_2021}. For estimating \acs*{DBH} in an experimental setup the system presents a \acs*{RMSE} of 0.07m not considering outliers and 0.14m with outliers. While their application has the benefit of having a built-in display that allows real-time visualization of data, one of its major drawbacks is the way they build their apparatus, opting for a metal stick rather than a backpack type of design.  As the authors acknowledge, user fatigue may lead to excessive variations in stick position, resulting in unintelligible and uncontrolled movements, damaging the performance of the apparatus. This would not be an issue if a more ergonomic structure was used. The errors due to user fatigue can be slightly mitigated by performing \acs{SLAM} in several sessions instead of one, allowing the user to rest between shorter sessions. After recording multiple sessions, \acs*{GPS} information is used to assemble the multiple sessions' map into a single map.

Yanjun Su et al. \cite{su_development_2021} developed an accurate backpack with dual \acs*{LiDAR} positioned orthogonally for forest inventory. This system consists of two Velodyne Puck VLP-16 \acs*{LiDAR}s, an \acs*{IMU}, and a display for viewing the generated point clouds in real time. The system was designed to not be \acs*{GPS} dependent. Using an open-source Python package, the researcher claims to have achieved an \acs*{RMSE} of 0.02m, a value slightly lower than Alexander's. Due to its vertical \acs*{LiDAR}, the system was also able to estimate the height of trees, obtaining a \acs*{RMSE} of 1.9m.

Sier et. al \cite{sier_benchmark_2022} design an apparatus with the objective to compute ground truth for trajectory in \acs*{GNSS} denied environments. The apparatus features six different \acs*{LiDAR}, with both spinning and solid state technologies and a stereo fish eye camera. The authors compare different state of the art \acs*{LiDAR} based \acs*{SLAM} with different \acs*{LiDAR} configurations to assess the best overall combination. The author concludes that the most robust combination is the FAST-LIO \cite{xu_fast-lio_2021} implementation using the more precise OS0 and OS1 spinning \acs*{LiDAR}s. It also concluded that on outdoor environment the solid state \acs*{LiDAR}s had a similar performance against the more expensive counterparts. This is to be expected as the number of features in outdoor environment is considerably larger.

A robot system developed by Jelavic performs precision harvesting missions \cite{jelavic_towards_2021}. The entire workflow begins with a human mapping the desired location with a \textit{sensor module}. A human expert then signals trees that need to be cut down to the autonomous vehicle. With the sensor module attached to the robot, a planning algorithm determines the most efficient path and pose to reach the next tree. Once the pose proposed in the previous step has been reached, the tree is grabbed and cut. In many ways, their \textit{sensor module} is similar to ours. Thei system uses a RealSense T265 tracking camera, an Ouster OS-1 \acs*{LiDAR}, an \acs*{IMU}, and a FLIR camera model BFS-U3-04S2M-CS, they spent approximately 10500 dollars. Cartographer was used to create the 3D map, with the \acs{LiDAR} being used for mapping and cameras for obtaining visual odometry. Only one of the cameras can be used at a time to obtain visual odometry. Similar to Proudman's design, the stick module may lead to user fatigue as discussed previously.

Table \ref*{tab: compare apparatus} offers a comparison between the several apparatus reviewed. As one is able to see, the apparatus focused in forest aplication don't compare multiple \acs*{SLAM} implementations, being more focused in determing the \acs*{DBH} of trees. Some of the forest focused apparatus send the recorded footage to an cloud base system to perform \acs*{SLAM} like GeoSLAM Hub\footnote{GeoSLAM Hub is a \acs*{SLAM} cloud provider, producing a map based on 3D data received. The \acs*{SLAM} algorithm used is not described}. Of all the implementations discussed only one uses a RGB-D camera, a critical component to perform combustible material identification and trees species and objects identification.

\begin{table}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{|c|ccccc|c|c|c|c|c|}
  \hline
  \multirow{2}{*}{\textbf{Implementation}} &
    \multicolumn{5}{c|}{\textbf{Inputs}} &
    \multirow{2}{*}{\textbf{Display}} &
    \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Structure\\ Type\end{tabular}}} &
    \multirow{2}{*}{\textbf{SLAM Used}} &
    \multirow{2}{*}{\textbf{Forestry Application}} &
    \multirow{2}{*}{\textbf{RMSE of DBH}} \\ \cline{2-6}
   &
    \multicolumn{1}{c|}{Stereo} &
    \multicolumn{1}{c|}{RGB-D} &
    \multicolumn{1}{c|}{IMU} &
    \multicolumn{1}{c|}{LiDAR} &
    GPS/GNSS &
     &
     &
     &
     &
     \\ \hline
  \begin{tabular}[c]{@{}c@{}}Oveland \cite{oveland_comparing_2018}\end{tabular} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    Yes &
    No &
    Backpack &
    GeoSLAM Hub &
    Yes &
    0.022m \\ \hline
  \begin{tabular}[c]{@{}c@{}}Xiao \cite{xiao_high-precision_2022}\end{tabular} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    Yes &
    Yes &
    Backpack &
    \begin{tabular}[c]{@{}c@{}}LeGO-LOAM, LIO-SAM,\\ HSDLIO\end{tabular} &
    No &
    N.A \\ \hline
  \begin{tabular}[c]{@{}c@{}}Proudman \cite{proudman_online_2021}\end{tabular} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    Yes &
    Yes &
    Stick &
    Graph-based SLAM &
    Yes &
    0.07m//0.14m \\ \hline
  \begin{tabular}[c]{@{}c@{}}Su \cite{su_development_2021}\end{tabular} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    No &
    Yes &
    Backpack &
    N.A &
    Yes &
    0.02 \\ \hline
  \begin{tabular}[c]{@{}c@{}}Sier \cite{sier_benchmark_2022}\end{tabular} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    Yes &
    No &
    Wheel Cart &
    \begin{tabular}[c]{@{}c@{}}LeGo-LOAM, FAST-LIO, \\ Livox-Mapping, LIO-LIVOX\end{tabular} &
    No &
    N.A \\ \hline
  \begin{tabular}[c]{@{}c@{}}Jevalic \cite{jelavic_towards_2021}\end{tabular} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{No} &
    \multicolumn{1}{c|}{Yes} &
    \multicolumn{1}{c|}{Yes} &
    No &
    No &
    Stick &
    Cartographer &
    Yes &
    N.A \\ \hline
  \end{tabular}%
  }
  \caption{Comparison table between previously mention systems.}
  \label{tab: compare apparatus}
\end{table}


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=0.8\linewidth]{images/background/alexandar_apparatus.jpg}
        \caption{Alexander's apparatus \cite{proudman_online_2021}.}
        \label{fig: Alexander apparatus}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=0.875\linewidth]{images/background/xiao_apparatus.jpg}
        \caption{Xiao's apparatus \cite{xiao_high-precision_2022}.}
        \label{fig: Xiao apparatus}
    \end{subfigure}
    \\[3ex]
    \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=\linewidth]{images/background/su_apparatus.jpg}
        \caption{Yanjun's apparatus \cite{su_development_2021}.}
        \label{fig: yanjun apparatus}
    \end{subfigure}
    \\[3ex]
    \begin{subfigure}{0.6\textwidth}
      \includegraphics[width=\linewidth]{images/background/jelavic_towards_sensor_module.png}
      \caption{Yanjun's apparatus \cite{su_development_2021}.}
      \label{fig: jelavic apparatus}
  \end{subfigure}
  \\[3ex]
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\linewidth]{images/background/oveland_apparatus.jpg}
        \caption{Oveland's apparatus \cite{oveland_comparing_2018}.}
        \label{fig: oveland apparatus}
    \end{subfigure}
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\linewidth]{images/background/sier_apparatus.png}
        \caption{Sier's apparatus \cite{sier_benchmark_2022}.}
        \label{fig: sier apparatus}
    \end{subfigure}
    \caption{Collection of the apparatus discussed in this section.}
    \label{fig: all apparatus}
\end{figure}